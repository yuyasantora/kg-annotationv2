from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from PIL import Image
import io
import numpy as np
import cv2
import torch
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict, Any
from pydantic import BaseModel
import os
from datetime import datetime
import urllib.request

# YOLOXã®ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from yolox.onnx_predictor import YOLOXONNXPredictor

app = FastAPI(title="KG Annotation AI Service", version="0.1.0")

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒ
yolox_predictor = None
sentence_model = None
vector_index = None

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    global yolox_predictor, sentence_model, vector_index
    
    print("ğŸš€ Loading AI models...")
    
    # YOLOXãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    try:
        # YOLOXã®ONNXãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        model_path = "yolox_s.onnx"
        if not os.path.exists(model_path):
            print("ğŸ“¥ Downloading YOLOX ONNX model...")
            model_url = "https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.onnx"
            urllib.request.urlretrieve(model_url, model_path)
            print("âœ… YOLOX ONNX model downloaded")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚¤ãƒˆå½¢å¼ã§èª­ã¿è¾¼ã¿
        with open(model_path, 'rb') as f:
            model_bytes = f.read()
        
        # YOLOXãƒ—ãƒ¬ãƒ‡ã‚£ã‚¯ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
        yolox_predictor = YOLOXONNXPredictor(
            model_bytes=model_bytes,
            input_shape_str="640,640"
        )
        print("âœ… YOLOX model loaded")
    except Exception as e:
        print(f"âŒ Failed to load YOLOX model: {e}")
    
    # Sentence Transformersãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    try:
        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… Sentence Transformer model loaded")
    except Exception as e:
        print(f"âŒ Failed to load Sentence Transformer: {e}")
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆæœŸåŒ–
    vector_index = faiss.IndexFlatIP(384)  # all-MiniLM-L6-v2ã®æ¬¡å…ƒæ•°
    print("âœ… Vector index initialized")

@app.get("/")
async def root():
    return {
        "message": "ğŸ KG Annotation AI Service",
        "status": "healthy",
        "version": "0.1.0",
        "models_loaded": {
            "yolox": yolox_predictor is not None,
            "sentence_transformer": sentence_model is not None,
            "vector_index": vector_index is not None
        }
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "models_status": {
            "yolox": "loaded" if yolox_predictor else "not_loaded",
            "sentence_transformer": "loaded" if sentence_model else "not_loaded",
            "vector_index": "initialized" if vector_index else "not_initialized"
        }
    }

@app.post("/detect")
async def detect_objects(image: UploadFile = File(...)):
    """YOLOXç‰©ä½“æ¤œå‡ºAPI"""
    if yolox_predictor is None:
        raise HTTPException(status_code=503, detail="YOLOX model not loaded")
    
    try:
        # ç”»åƒèª­ã¿è¾¼ã¿
        image_bytes = await image.read()
        pil_image = Image.open(io.BytesIO(image_bytes))
        
        # RGBâ†’BGRã«å¤‰æ›ï¼ˆOpenCVå½¢å¼ï¼‰
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        # PIL â†’ numpy â†’ OpenCV BGR
        img_array = np.array(pil_image)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        
        # YOLOXæ¨è«–å®Ÿè¡Œ
        detections_raw = yolox_predictor.predict(
            origin_img_bgr=img_bgr,
            score_thr=0.3,
            nms_thr=0.45
        )
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã®å½¢å¼ã«å¤‰æ›
        detections = []
        for i, det in enumerate(detections_raw):
            detections.append({
                "id": i,
                "class_name": det["label_name"],
                "confidence": round(det["score"], 3),
                "bbox": {
                    "x1": float(det["xmin"]),
                    "y1": float(det["ymin"]),
                    "x2": float(det["xmax"]),
                    "y2": float(det["ymax"])
                }
            })
        
        return {
            "success": True,
            "image_info": {
                "filename": image.filename,
                "size": pil_image.size,
                "format": pil_image.format
            },
            "detections": detections,
            "total_objects": len(detections)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Detection failed: {str(e)}")

@app.post("/vectorize")
async def vectorize_text(texts: List[str]):
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–API"""
    if sentence_model is None:
        raise HTTPException(status_code=503, detail="Sentence Transformer model not loaded")
    
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embeddings = sentence_model.encode(texts, convert_to_tensor=False)
        
        return {
            "success": True,
            "vectors": embeddings.tolist(),
            "dimension": embeddings.shape[1],
            "total_texts": len(texts)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Vectorization failed: {str(e)}")

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‹ã‚’å®šç¾©
class SearchRequest(BaseModel):
    query_vector: List[float]
    vectors: List[List[float]]  # è¤‡æ•°ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å—ã‘å–ã‚‹
    top_k: int = 5

@app.post("/search_similar")
async def search_similar_images(request: SearchRequest):
    """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢API"""
    try:
        # æ¤œç´¢ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
        dimension = len(request.query_vector)
        index = faiss.IndexFlatIP(dimension)  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
        vectors = np.array(request.vectors).astype(np.float32)
        index.add(vectors)
        
        # æ¤œç´¢å®Ÿè¡Œ
        query_vector = np.array([request.query_vector]).astype(np.float32)
        scores, indices = index.search(query_vector, min(request.top_k, len(request.vectors)))
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx != -1:  # æœ‰åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                results.append({
                    "rank": i + 1,
                    "index": int(idx),
                    "similarity": float(score)
                })
        
        return {
            "success": True,
            "query_dimension": dimension,
            "results": results,
            "total_found": len(results)
        }
        
    except Exception as e:
        print(f"âŒ Search error: {e}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

if __name__ == "__main__":
    print("ğŸ KG Annotation AI Service starting...")
    uvicorn.run(app, host="0.0.0.0", port=8001)  # ãƒãƒ¼ãƒˆã‚’8001ã«å¤‰æ›´