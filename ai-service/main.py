from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from PIL import Image
import io
import numpy as np
import cv2
from sentence_transformers import SentenceTransformer
import faiss
from typing import List
from pydantic import BaseModel
import os
from datetime import datetime
import urllib.request
import traceback

# YOLOXã®ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from yolox.onnx_predictor import YOLOXONNXPredictor

app = FastAPI(title="KG Annotation AI Service", version="0.2.0")

# ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ã‚’è¿½åŠ 
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    print(f"âŒ Validation error: {exc.errors()}")
    return JSONResponse(
        status_code=422,
        content={
            "detail": exc.errors(),
            # FormDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãã®ã¾ã¾å«ã‚ã‚‹ã®ã§ã¯ãªãã€å¿…è¦ãªæƒ…å ±ã ã‘ã‚’æŠ½å‡º
            "error_summary": str(exc)
        },
    )

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒ
yolox_predictor = None
clip_model = None

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    global yolox_predictor, clip_model
    
    print("ğŸš€ Loading AI models...")
    
    # YOLOXãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ (æ—¢å­˜ã®ã¾ã¾)
    try:
        model_path = "yolox_s.onnx"
        if not os.path.exists(model_path):
            print("ğŸ“¥ Downloading YOLOX ONNX model...")
            model_url = "https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.onnx"
            urllib.request.urlretrieve(model_url, model_path)
            print("âœ… YOLOX ONNX model downloaded")
        
        with open(model_path, 'rb') as f:
            model_bytes = f.read()
        
        yolox_predictor = YOLOXONNXPredictor(model_bytes=model_bytes, input_shape_str="640,640")
        print("âœ… YOLOX model loaded")
    except Exception as e:
        print(f"âŒ Failed to load YOLOX model: {e}")
    
    # Sentence Transformersãƒ¢ãƒ‡ãƒ«ã‚’CLIPãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´
    try:
        # ãƒ¢ãƒ‡ãƒ«ã‚’'clip-ViT-B-32'ã«å¤‰æ›´
        clip_model = SentenceTransformer('clip-ViT-B-32')
        print("âœ… CLIP model (clip-ViT-B-32) loaded")
    except Exception as e:
        print(f"âŒ Failed to load CLIP model: {e}")
    
@app.get("/")
async def root():
    return {
        "message": "ğŸ KG Annotation AI Service",
        "status": "healthy",
        "version": "0.2.0",
        "models_loaded": {
            "yolox": yolox_predictor is not None,
            "clip_model": clip_model is not None,
        }
    }

# --- ã“ã“ã‹ã‚‰æ–°ã—ã„APIã¨ä¿®æ­£ã•ã‚ŒãŸAPI ---

@app.post("/vectorize_image")
async def vectorize_image(
    image: UploadFile = File(..., description="The image file to vectorize")
):
    """ç”»åƒã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹API"""
    if clip_model is None:
        raise HTTPException(status_code=503, detail="CLIP model not loaded")
    
    try:
        # å…¥åŠ›ãƒã‚§ãƒƒã‚¯
        if not image:
            raise HTTPException(status_code=422, detail="No image file provided")
        
        print(f"ğŸ“¥ Received image: {image.filename}, content_type: {image.content_type}")
        
        # content_typeãƒã‚§ãƒƒã‚¯
        if not image.content_type or not image.content_type.startswith('image/'):
            raise HTTPException(
                status_code=422,
                detail=f"Invalid content type: {image.content_type}. Expected image/*"
            )
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        try:
            image_bytes = await image.read()
            if not image_bytes:
                raise ValueError("Empty image data")
            print(f"ğŸ“Š Image size: {len(image_bytes)} bytes")
        except Exception as e:
            print(f"âŒ Failed to read image data: {e}")
            raise HTTPException(status_code=422, detail=f"Failed to read image data: {str(e)}")
        
        # PILã§ç”»åƒã‚’é–‹ã
        try:
            pil_image = Image.open(io.BytesIO(image_bytes))
            print(f"ğŸ–¼ Image opened: size={pil_image.size}, mode={pil_image.mode}")
        except Exception as e:
            print(f"âŒ Failed to open image: {e}")
            raise HTTPException(status_code=422, detail=f"Failed to open image: {str(e)}")
        
        # RGBã«å¤‰æ›
        try:
            if pil_image.mode != 'RGB':
                print(f"ğŸ”„ Converting image from {pil_image.mode} to RGB")
                pil_image = pil_image.convert('RGB')
        except Exception as e:
            print(f"âŒ Failed to convert image to RGB: {e}")
            raise HTTPException(status_code=422, detail=f"Failed to convert image to RGB: {str(e)}")
        
        # ç”»åƒã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        try:
            print("ğŸ§® Starting image vectorization...")
            embedding = clip_model.encode(pil_image, convert_to_tensor=False)
            print(f"âœ… Vectorization successful: shape={embedding.shape}")
        except Exception as e:
            print(f"âŒ Vectorization failed: {e}")
            print(f"âŒ Traceback: {traceback.format_exc()}")
            raise HTTPException(status_code=500, detail=f"Vectorization failed: {str(e)}")
        
        return {
            "success": True,
            "vector": embedding.tolist(),
            "dimension": embedding.shape[0]
        }
    except HTTPException:
        raise
    except Exception as e:
        error_detail = f"Unexpected error during image vectorization: {str(e)}"
        print(f"âŒ Error: {error_detail}")
        print(f"âŒ Error type: {type(e)}")
        print(f"âŒ Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=error_detail)


@app.post("/vectorize_text")
async def vectorize_text(texts: List[str]):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹API (æ—§/vectorize)"""
    if clip_model is None:
        raise HTTPException(status_code=503, detail="CLIP model not loaded")
    
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embeddings = clip_model.encode(texts, convert_to_tensor=False)
        
        return {
            "success": True,
            "vectors": embeddings.tolist(),
            "dimension": embeddings.shape[1],
            "total_texts": len(texts)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Text vectorization failed: {str(e)}")

class SearchRequest(BaseModel):
    query_vector: List[float]
    vectors: List[List[float]]
    ids: List[str] # ç”»åƒIDã®ãƒªã‚¹ãƒˆã‚’è¿½åŠ 
    top_k: int = 10

@app.post("/search_similar_images")
async def search_similar_images(request: SearchRequest):
    """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢API (æ—§/search_similar)"""
    if len(request.vectors) == 0 or len(request.ids) == 0:
        return {"success": True, "results": []}
    if len(request.vectors) != len(request.ids):
        raise HTTPException(status_code=400, detail="Length of vectors and ids must be the same")

    try:
        dimension = len(request.query_vector)
        # CLIPã®ãƒ™ã‚¯ãƒˆãƒ«é•·(512)ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèª
        if dimension != 512:
             print(f"âš ï¸ Warning: Query vector dimension is {dimension}, but CLIP model expects 512.")

        index = faiss.IndexFlatIP(dimension)
        
        vectors_np = np.array(request.vectors).astype('float32')
        faiss.normalize_L2(vectors_np) # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—ã®ãŸã‚ã«æ­£è¦åŒ–
        index.add(vectors_np)
        
        query_vector_np = np.array([request.query_vector]).astype('float32')
        faiss.normalize_L2(query_vector_np) # åŒæ§˜ã«æ­£è¦åŒ–
        
        scores, indices = index.search(query_vector_np, min(request.top_k, len(request.vectors)))
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1:
                results.append({
                    "id": request.ids[idx], # æ•°å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä»£ã‚ã‚Šã«ç”»åƒIDã‚’è¿”ã™
                    "similarity": float(score)
                })
        
        return {"success": True, "results": results}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

# --- YOLOXã®API (æ—¢å­˜ã®ã¾ã¾) ---
@app.post("/detect")
async def detect_objects(image: UploadFile = File(...)):
    if yolox_predictor is None:
        raise HTTPException(status_code=503, detail="YOLOX model not loaded")
    
    try:
        image_bytes = await image.read()
        pil_image = Image.open(io.BytesIO(image_bytes))
        
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        img_array = np.array(pil_image)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        
        detections_raw = yolox_predictor.predict(origin_img_bgr=img_bgr, score_thr=0.3, nms_thr=0.45)
        
        detections = []
        for i, det in enumerate(detections_raw):
            detections.append({
                "id": i,
                "class_name": det["label_name"],
                "confidence": round(det["score"], 3),
                "bbox": {
                    "x1": float(det["xmin"]),
                    "y1": float(det["ymin"]),
                    "x2": float(det["xmax"]),
                    "y2": float(det["ymax"])
                }
            })
        
        return {"success": True, "detections": detections}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Detection failed: {str(e)}")


if __name__ == "__main__":
    print("ğŸ KG Annotation AI Service starting...")
    # ãƒãƒ¼ãƒˆã‚’8001ã«å¤‰æ›´
    uvicorn.run(app, host="0.0.0.0", port=8001)